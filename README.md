The py spark code for home sales sample data in AWS was analyzed in Google colab. Link to code and comments navigating through the process can be found in the starter_code/home_sales_colab.ipnyb file.

The syntax for this file was created with the assistance of gpt4o. 

I find it interesting that the partitioned data took longer to process than the non-cached and chaced data, but this could be due to adding additional "compartmentalization" on the data, which would be more effective with larger datasets opposed to the sample we have. 
